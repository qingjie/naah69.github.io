<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MATH on </title>
    <link>https://www.naah69.com/categories/math/</link>
    <description>Recent content in MATH on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 13 Feb 2020 15:06:33 +0800</lastBuildDate>
    
	<atom:link href="https://www.naah69.com/categories/math/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>深度学习数学基础(六)之链式法则</title>
      <link>https://www.naah69.com/post/2020-02-11-linked/</link>
      <pubDate>Thu, 13 Feb 2020 15:06:33 +0800</pubDate>
      
      <guid>https://www.naah69.com/post/2020-02-11-linked/</guid>
      <description>上次讲了导数和偏导数的基础，那么这些还不足以使用起来，今天就来讲讲误差反向传播中用来解决复杂函数求导的链式法则。 1 复合函数 已知函数$y=f(</description>
    </item>
    
    <item>
      <title>深度学习数学基础(五)之导数基础</title>
      <link>https://www.naah69.com/post/2020-02-10-derivative/</link>
      <pubDate>Mon, 10 Feb 2020 11:29:17 +0800</pubDate>
      
      <guid>https://www.naah69.com/post/2020-02-10-derivative/</guid>
      <description>之前我们提过，说神经网络会很牛逼的自己学习，那这个自己学习是什么原理呢？如果用数学上的非人话就是对权重和偏置进行最优化，使输出符合学习数据，</description>
    </item>
    
    <item>
      <title>深度学习数学基础(四)之线性代数基础</title>
      <link>https://www.naah69.com/post/2020-02-09-linear-algebra/</link>
      <pubDate>Sun, 09 Feb 2020 18:20:23 +0800</pubDate>
      
      <guid>https://www.naah69.com/post/2020-02-09-linear-algebra/</guid>
      <description>1 向量基础 向量就是具有大小和方向的量。 1.1 有向线段与向量 有两个点$A、B$，我们考虑从$A$指向$B$的线段，这条具有方向的线段$AB$叫作有</description>
    </item>
    
    <item>
      <title>深度学习数学基础(三)之简单数学</title>
      <link>https://www.naah69.com/post/2020-02-08-math-basic/</link>
      <pubDate>Sat, 08 Feb 2020 14:13:58 +0800</pubDate>
      
      <guid>https://www.naah69.com/post/2020-02-08-math-basic/</guid>
      <description>从本文开始，之后的三四篇我们都将沐浴在数学的海洋里，拼命地扑腾，这个系列我会尽力以通俗易懂的方式来讲述这些数学知识。 1 函数 1.1 一次函数 在数学函</description>
    </item>
    
    <item>
      <title>深度学习数学基础(二)之神经网络构造</title>
      <link>https://www.naah69.com/post/2020-02-06-neural-network-structure/</link>
      <pubDate>Thu, 06 Feb 2020 19:40:19 +0800</pubDate>
      
      <guid>https://www.naah69.com/post/2020-02-06-neural-network-structure/</guid>
      <description>书接上文，继续说说神经网络的结构。首先我们要回顾上一篇文章深度学习数学基础(一)之神经元构造中讲到的两个公式。 加权输入公式 $$z=w_1x_1+w_2x_2+w_3x_3+&amp;hellip;+b $$ 激活公式 $$y=a(z)$$ 还有</description>
    </item>
    
    <item>
      <title>深度学习数学基础(一)之神经元构造</title>
      <link>https://www.naah69.com/post/2020-02-06-neurous/</link>
      <pubDate>Thu, 06 Feb 2020 10:46:49 +0800</pubDate>
      
      <guid>https://www.naah69.com/post/2020-02-06-neurous/</guid>
      <description>过年期间，我抽时间把深度学习的神经网络的数学基础学习了一下，简单的看了看神经网络的基础，在这里我通过写这个系列的博文来沉淀我所学到的知识。 1</description>
    </item>
    
    <item>
      <title>《什么是数学》的思维导图</title>
      <link>https://www.naah69.com/post/2018-12-01-math-mind/</link>
      <pubDate>Sat, 01 Dec 2018 14:25:38 +0800</pubDate>
      
      <guid>https://www.naah69.com/post/2018-12-01-math-mind/</guid>
      <description>1 前言 由于要转向AI的算法岗位，所以最近在恶补数学，有幸拜读了一本，国外很火的数学基础书《什么是数学》。 也将过去忘记的数学简单的复习了一下，</description>
    </item>
    
  </channel>
</rss>